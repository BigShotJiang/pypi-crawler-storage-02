{
  "name": "pi-flash-attn",
  "version": "2.8.2",
  "summary": "Flash Attention: Fast and Memory-Efficient Exact Attention",
  "author": "Tri Dao",
  "license": null,
  "home_page": "https://github.com/Dao-AILab/flash-attention",
  "download_filename": "pi_flash_attn-2.8.2.tar.gz",
  "download_time": "2025-08-11T23:25:25.201843",
  "package_url": "https://pypi.org/project/pi-flash-attn/"
}